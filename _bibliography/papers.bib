---
---

@inproceedings{shyam2022generic,
  title={GIQE: Generic Image Quality Enhancement via N$^{th}$ Order Iterative Degradation},
  author={Shyam, Pranjay and Yoon, Kuk-Jin and Kim, Kyung-Soo},
  abbr={CVPR},
  year={2022},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  abstract={Visual degradations caused by motion blur, raindrop, rain, snow, illumination, and fog deteriorate image quality and, subsequently, the performance of perception algorithms deployed in outdoor conditions. While degradation-specific image restoration techniques have been extensively studied, such algorithms are domain sensitive and fail in real scenarios where multiple degradations exist simultaneously. This makes a case for blind image restoration and reconstruction algorithms as practically relevant. However, the absence of a dataset diverse enough to encapsulate all variations hinders development for such an algorithm. In this paper, we utilize a synthetic degradation model that recursively applies sets of random degradations to generate naturalistic degradation images of varying complexity, which are used as input. Furthermore, as the degradation intensity can vary across an image, the spatially invariant convolutional filter cannot be applied for all degradations. Hence to enable spatial variance during image restoration and reconstruction, we design a transformer-based architecture to benefit from the long-range dependencies. In addition, to reduce the computational cost of transformers, we propose a multi-branch structure coupled with modifications such as a complimentary feature selection mechanism and the replacement of a feed-forward network with lightweight multiscale convolutions. Finally, to improve restoration and reconstruction, we integrate an auxiliary decoder branch to predict the degradation mask to ensure the underlying network can localize the degradation information. From empirical analysis on 10 datasets covering rain drop removal, deraining, dehazing, image enhancement, and deblurring, we demonstrate the efficacy of the proposed approach while obtaining SoTA performance. },
}

@inproceedings{shyam2022dgss,
  title={DGSS : Domain Generalized Semantic Segmentation using Iterative Style Mining and Latent Representation Alignment},
  author={Shyam, Pranjay and Bangunharcana, Antyanta and Kim, Kyung-Soo},
  journal={arXiv},
  abbr={arXiv},
  year={2022},
  abstract={Semantic segmentation algorithms require access to well-annotated datasets captured under diverse illumination conditions to ensure consistent performance. However, poor visibility conditions at varying illumination conditions result in laborious and error-prone labeling. Alternatively, using synthetic samples to train segmentation algorithms has gained interest with the drawback of domain gap that results in sub-optimal performance. While current state-of-the-art (SoTA) have proposed different mechanisms to bridge the domain gap, they still perform poorly in low illumination conditions with an average performance drop of - 10.7 mIOU. In this paper, we focus upon single source domain generalization to overcome the domain gap and propose a two-step framework wherein we first identify an adversarial style that maximizes the domain gap between stylized and source images. Subsequently, these stylized images are used to categorically align features such that features belonging to the same class are clustered together in latent space, irrespective of domain gap. Furthermore, to increase intra-class variance while training, we propose a style mixing mechanism wherein the same objects from different styles are mixed to construct a new training image. This framework allows us to achieve a domain generalized semantic segmentation algorithm with consistent performance without prior information of the target domain while relying on a single source. Based on extensive experiments, we match SoTA performance on SYNTHIA $\to$ Cityscapes, GTAV $\to$ Cityscapes while setting new SoTA on GTAV $\to$ Dark Zurich and GTAV $\to$ Night Driving benchmarks without retraining.},
  arxiv={https://arxiv.org/abs/2202.13144},
}

@article{hong2022robotic,
  title={Robotic Mapping Approach under Illumination-Variant Environments at Planetary Construction Sites},
  author={Hong, Sungchul and Shyam, Pranjay and Bangunharcana, Antyanta and Shin, Hyuseoung},
  journal={Remote Sensing},
  abbr={RS},
  volume={14},
  number={4},
  pages={1027},
  year={2022},
  publisher={Multidisciplinary Digital Publishing Institute},
  abstract={With the recent discovery of water-ice and lava tubes on the Moon and Mars along with the development of in-situ resource utilization (ISRU) technology, the recent planetary exploration has focused on rover (or lander)-based surface missions toward the base construction for long-term human exploration and habitation. However, a 3D terrain map, mostly based on orbitersâ€™ terrain images, has insufficient resolutions for construction purposes. In this regard, this paper introduces the visual simultaneous localization and mapping (SLAM)-based robotic mapping method employing a stereo camera system on a rover. In the method, S-PTAM is utilized as a base framework, with which the disparity map from the self-supervised deep learning is combined to enhance the mapping capabilities under homogeneous and unstructured environments of planetary terrains. The overall performance of the proposed method was evaluated in the emulated planetary terrain and validated with potential results.},
  paper={https://www.mdpi.com/1424-8220/21/22/7715}
}

@inproceedings{shyam2021towards,
  title={Towards Domain Invariant Single Image Dehazing},
  author={Shyam, Pranjay and Yoon, Kuk-Jin and Kim, Kyung-Soo},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={11},
  pages={9657--9665},
  year={2021},
  abbr={AAAI},
  category={conference},
  abstract={Presence of haze in images obscures underlying information, which is undesirable in applications requiring accurate environment information. To recover such an image, a dehazing algorithm should localize and recover affected regions while ensuring consistency between recovered and its neighboring regions. However owing to fixed receptive field of convolutional kernels and non uniform haze distribution, assuring consistency between regions is difficult. In this paper, we utilize an encoder-decoder based network architecture to perform the task of dehazing and integrate an spatially aware channel attention mechanism to enhance features of interest beyond the receptive field of traditional conventional kernels. To ensure performance consistency across diverse range of haze densities, we utilize greedy localized data augmentation mechanism. Synthetic datasets are typically used to ensure a large amount of paired training samples, however the methodology to generate such samples introduces a gap between them and real images while accounting for only uniform haze distribution and overlooking more realistic scenario of non-uniform haze distribution resulting in inferior dehazing performance when evaluated on real datasets. Despite this, the abundance of paired samples within synthetic datasets cannot be ignored. Thus to ensure performance consistency across diverse datasets, we train the proposed network within an adversarial prior-guided framework that relies on a generated image along with its low and high frequency components to determine if properties of dehazed images matches those of ground truth. We preform extensive experiments to validate the dehazing and domain invariance performance of proposed framework across diverse domains and report state-of-the-art (SoTA) results.},
  pdf={https://www.aaai.org/AAAI21Papers/AAAI-1706.ShyamP.pdf},
}

@inproceedings{shyam2021lightweight,
  title={Lightweight hdr camera isp for robust perception in dynamic illumination conditions via fourier adversarial networks},
  author={Shyam, Pranjay and Yoon, Kuk-Jin and Sengar, Sandeep Singh and Kim, Kyung-Soo},
  booktitle={The 32nd British Machine Vision Conference, BMVC},
  year={2021},
  organization={British Machine Vision Association (BMVA)},
  abbr={BMVC},
  category={conference},
  abstract={The limited dynamic range of commercial compact camera sensors results in an inac-
curate representation of scenes with varying illumination conditions, adversely affecting
image quality and subsequently limiting the performance of underlying image process-
ing algorithms. Current state-of-the-art (SoTA) convolutional neural networks (CNN) are
developed as post-processing techniques to independently recover under-/over-exposed
images. However, when applied to images containing real-world degradations such as
glare, high-beam, color bleeding with varying noise intensity, these algorithms amplify
the degradations, further degrading image quality. We propose a lightweight two-stage
image enhancement algorithm sequentially balancing illumination and noise removal us-
ing frequency priors for structural guidance to overcome these limitations. Furthermore,
to ensure realistic image quality, we leverage the relationship between frequency and
spatial domain properties of an image and propose a Fourier spectrum-based adversar-
ial framework (AFNet) for consistent image enhancement under varying illumination
conditions. While current formulations of image enhancement are envisioned as post-
processing techniques, we examine if such an algorithm could be extended to integrate
the functionality of the Image Signal Processing (ISP) pipeline within the camera sensor
benefiting from RAW sensor data and lightweight CNN architecture. Based on quanti-
tative and qualitative evaluations, we also examine the practicality and effects of image
enhancement techniques on the performance of common perception tasks such as object
detection and semantic segmentation in varying illumination conditions},
  pdf={https://www.bmvc2021-virtualconference.com/assets/papers/1406.pdf},
}

@InProceedings{shyam2021evaluating,
  title={Evaluating COPY-BLEND Augmentation for Low Level Vision Tasks},
  author={Shyam, Pranjay and Sengar, Sandeep Singh and Yoon, Kuk-Jin and Kim, Kyung-Soo},
  journal={arXiv preprint arXiv:2103.05889},
  year={2021},
  abbr={IJCAI-W},
  category={conference},
  abstract={Image restoration and enhancement algorithms aim to improve image quality for ensuring consistent performance of high level vision tasks such as object detection, semantic segmentation, image classification, etc in different adverse conditions. State-of-the-art (SoTA) approaches have relied on brute force combination of large scale datasets, deep convolutional neural networks (CNNs) and data augmentation techniques to achieve a peak performance. However a critical question that remains unanswered is the amount of paired training dataset actually needed to reach peak performance, this is extremely critical as capturing a paired dataset corresponding to different degradations is time consuming. In this paper we focus on region modification based data augmentation techniques as well as discriminator augmentation to examine if these can improve performance of low level vision tasks such as image enhancement, dehazing, deraining, deblurring, super resolution and denoising. The primary objective being in ensuring that an underlying algorithm localizes, identifies quantum of recovery and subsequently recovers affected regions resulting in increased perceptual quality of a recovered image. From extensive experiments we summarize intriguing properties such as improved higher performance from lower amounts of training data without any modification to baseline algorithms},
  paper={https://arxiv.org/abs/2103.05889},
}

@InProceedings{Shyam_2021_ICCV,
    author    = {Shyam, Pranjay and Yoon, Kuk-Jin and Kim, Kyung-Soo},
    title     = {Weakly Supervised Approach for Joint Object and Lane Marking Detection},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
    month     = {October},
    year      = {2021},
    pages     = {2885-2895},
  abbr={ICCV-W},
  category={conference},
  abstract={Understanding the driving scene is critical for the safe operation of autonomous vehicles with state-of-the-art (SoTA) systems relying upon a combination of different algorithms to perform tasks for mathematically representing an environment. Amongst these tasks, lane and object detection are highly popular and have been extensively researched independently. However, their joint operation is rarely studied primarily due to the lack of a dataset that captures these attributes together, resulting in increased redundant computations that can be eliminated simply by performing these tasks together. To overcome this, we propose a weakly-supervised approach wherein, given an image from the lane detection dataset, we use a pretrained network to label different objects within a scene, generating pseudo bounding boxes used to train a network that jointly detects objects and lane lines. With an emphasis on inference speed and performance, we utilize prior works to construct two architectures based on Convolutional Neural Networks (CNNs) and Transformers. The CNN-based approach uses row-based pixel classification to detect and cluster lane lines alongside a single-stage anchor free object detector while sharing the same encoder backbone. Alternatively, using dual decoders, the transformer-based approach directly estimates bounding boxes and polynomial coefficients of lane lines. Through extensive qualitative and quantities experiments, we demonstrate the efficacy of the proposed architectures on leading datasets for object and lane detections and report state-of-the-art (SoTA) performance per GFLOPs.},
  pdf={https://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Shyam_Weakly_Supervised_Approach_for_Joint_Object_and_Lane_Marking_Detection_ICCVW_2021_paper.pdf},
}

@INPROCEEDINGS{9561632,
  author={Shyam, Pranjay and Yoon, Kuk-Jin and Kim, Kyung-Soo},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Adversarially-trained Hierarchical Feature Extractor for Vehicle Re-identification}, 
  year={2021},
  volume={},
  number={},
  pages={13400-13407},
  doi={10.1109/ICRA48506.2021.9561632},
  abbr={ICRA},
  category={conference},
  abstract={Vehicle Re-identification (Re-ID) aims to retrieve all instances of query vehicle images present in an image pool. However viewpoint, illumination, and occlusion variations along with subtle differences between two unique images pose a significant challenge towards achieving an effective system. In this paper, we emphasize upon enhancing the performance of visual feature based ReID system by improving feature embedding quality and propose (1) an attention-guided hierarchical feature extractor (HFE) that leverages the structure of a backbone CNN to extract coarse and fine-grained features and (2) to train the proposed network within a hard negative adversarial framework that generates samples exhibiting extreme variations, encouraging the network to extract important distinguishing features across varying scales. To demonstrate the effectiveness of the proposed framework we use VERI-Wild, VRIC and Veri-776 datasets that exhibit extreme intra-class and minute inter-class differences and achieve state-of-the-art (SoTA) performance.},
  paper={https://ieeexplore.ieee.org/document/9561632},
}

@INPROCEEDINGS{9268426,
  author={Shyam, Pranjay and *Bangunharcana, Antyanta and Kim, Kyung-Soo},
  booktitle={2020 20th International Conference on Control, Automation and Systems (ICCAS)}, 
  title={Retaining Image Feature Matching Performance Under Low Light Conditions}, 
  year={2020},
  volume={},
  number={},
  pages={1079-1085},
  doi={10.23919/ICCAS50221.2020.9268426},
  abbr={ICCAS},
  category={conference},
  abstract={Poor image quality in low light images may result in a reduced number of feature matching between images. In this paper, we investigate the performance of feature extraction algorithms in low light environments. To find an optimal setting to retain feature matching performance in low light images, we look into the effect of changing feature acceptance threshold for feature detector and adding pre-processing in the form of Low Light Image Enhancement (LLIE) prior to feature detection. We observe that even in low light images, feature matching using traditional hand-crafted feature detectors still performs reasonably well by lowering the threshold parameter. We also show that applying LLIE algorithms can improve feature matching even further when paired with the right feature extraction algorithm.},
  paper={https://ieeexplore.ieee.org/document/9268426},
}

@INPROCEEDINGS{9197076,
  author={Shyam, Pranjay and Yoon, Kuk-Jin and Kim, Kyung-Soo},
  booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Dynamic Anchor Selection for Improving Object Localization}, 
  year={2020},
  volume={},
  number={},
  pages={9477-9483},
  doi={10.1109/ICRA40945.2020.9197076},
  abbr={ICRA},
  category={conference},
  abstract={Anchor boxes act as potential object localization candidates allow single-stage detectors to achieve real-time performance, at the cost of localization accuracy when compared to state-of-the-art two-stage detectors. Therefore, correct selection of the scale and aspect ratio associated with an anchor box is crucial for detector performance. In this work, we propose a novel architecture called DANet for improving the localization performance of single-stage object detectors, while maintaining real-time inference. The proposed network achieves this by predicting (1) the combination of aspect ratio and scale per feature map based on object density and (2) localization confidence per anchor box. We evaluate the proposed network using the benchmark dataset. On the MS COCO dataset, DANet achieves 30.9\% AP at 51.8 fps using ResNet-18 and 45.3\% AP at 7.4 fps using ResNeXt-101.},
  paper={https://ieeexplore.ieee.org/document/9197076},
}
