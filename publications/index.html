<!DOCTYPE html>
<html lang="">

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Pranjay Shyam


  | Publications

</title>
<meta name="description" content="Ph.D. Candidate, Robust Perception for Autonomous Systems - Korea Advanced Institute of Science and Technology (KAIST)">

<!-- Open Graph -->

<meta property="og:site_name" content="Ph.D. Candidate, Robust Perception for Autonomous Systems - Korea Advanced Institute of Science and Technology (KAIST)" />
<meta property="og:type" content="object" />
<meta property="og:title" content="" />
<meta property="og:url" content="https://pranjay-dev.github.io/publications/" />
<meta property="og:description" content="Publications" />
<meta property="og:image" content="" />


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="https://pranjay-dev.github.io/publications/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
  <div class="container">
    
    <a class="navbar-brand title font-weight-lighter" href="https://pranjay-dev.github.io/">
     <span class="font-weight-bold">Pranjay</span>   Shyam
    </a>
    
    <!-- Navbar Toogle -->
    <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="sr-only">Toggle navigation</span>
      <span class="icon-bar top-bar"></span>
      <span class="icon-bar middle-bar"></span>
      <span class="icon-bar bottom-bar"></span>
    </button>
    <div class="collapse navbar-collapse text-right" id="navbarNav">
      <ul class="navbar-nav ml-auto flex-nowrap">
        <!-- About -->
        <li class="nav-item ">
          <a class="nav-link" href="/">
            Home
            
          </a>
        </li>
        
        <!-- Other pages -->
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <li class="nav-item active">
            <a class="nav-link" href="/publications/">
              Publications
              
              <span class="sr-only">(current)</span>
              
            </a>
        </li>
        
        
        
          <div class="toggle-container">
            <a id="light-toggle">
                <i class="fas fa-moon"></i>
                <i class="fas fa-sun"></i>
            </a>
          </div>
        
      </ul>
    </div>
  </div>
</nav>

</header>

    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Publications</h1>
    <p class="post-description">Publications in reverse chronological order  * denotes equal contribution</p>
  </header>

  <article>
    <div class="publications">
<!-- * denotes equal contribution -->
<!-- <h1> preprints </h1> -->


  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IV</abbr>
    
  
  </div>

  <div id="shyam2022infra" class="col-sm-8">
    
      <div class="title">Infra Sim-to-Real : An efficient baseline and dataset for Infrastructure based Online Object Detection and Tracking using Domain Adaptation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Shyam, Pranjay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Mishra, Sumit,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yoon, Kuk-Jin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kim, Kyung-Soo
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 2022 IEEE Intelligent Vehicles Symposium</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Increasing usage of traffic cameras provides an opportunity to utilize them for smart city applications. However, the efficacy of such systems is determined by their ability to detect and track objects of interest from diverse viewpoints accurately. This is challenging due to the diverse viewpoints, elevations, and distinct properties of camera sensors. Thus, to ensure robust performance, the training dataset should cover many variations, including viewpoints, illumination changes, and diverse weather conditions. However, constructing such a dataset is expensive in terms of data collection and annotation. This paper proposes an unsupervised domain adaptation approach wherein a synthetic dataset is generated using a simulator and subsequently used to ensure performance consistency of multi-object-tracking (MOT) algorithms across a diverse range of manually annotated natural scenes. Towards this end, we emphasize achieving domain invariant object detection by combining image stylization and class-balancing augmentation. Furthermore, we extend the robust detection algorithm to track detected objects across a large time scale using feature embeddings generated by the detector. Based on qualitative and quantitative results, we demonstrate the viability of such a system that is invariant to illumination, weather, viewpoint, and scene changes while providing a baseline for future research.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="shyam2022mtsp" class="col-sm-8">
    
      <div class="title">Holisitc Scene Understanding from Disjoint Labels via Multi-Task Stereo Perception</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Shyam, Pranjay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yoon, Kuk-Jin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kim, Kyung-Soo
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In arXiv</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The perception system of an autonomous vehicle relies on the computation of multiple attributes to mathematically represent surrounding information. Presently these attributes are computed independently using task-specific networks, which result in high computational costs. While the alternative of using a multi-task formulation is highly desirable given its ability to reduce the computational cost, a significant caveat restricting the deployment of such algorithms is their requirement of aligned multi-task labels corresponding to each image. This paper explores the realm of training a multi-task network using pseudo labels generated by task-specific networks using disjoint labels from (1) same and (2) different distributions. Specifically given a task with the labeled dataset, we train teacher models and generate aligned pseudo labels for an unlabelled dataset. These pseudo labels are used to train a multi-task network in a supervised manner. Given this training framework, we evaluate the performance of a baseline multi-task stereo network for performing scene representation using 6 tasks comprising coarse (2D detection) and dense predictions (depth estimation, semantic segmentation, road attribute segmentation, instance segmentation, and panoptic segmentation). We empirically demonstrate this approach to outperform teacher networks highlighting strong scene understanding capability supported by learning a common feature representation. We also demonstrate such networks to demonstrate domain invariant performance, presenting a promising direction to train multi-task learning algorithms using an unlabelled training dataset.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="shyam2022self_sup" class="col-sm-8">
    
      <div class="title">Enhancing Self-Supervised Monocular Depth Estimation via Feature Enhancement and Geometric Constraints</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Shyam, Pranjay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yoon, Kuk-Jin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kim, Kyung-Soo
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In arXiv</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
      <a href="https://github.com/pranjay-dev/SS_DS" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Self-Supervised learning provides an alternative for training monocular depth estimation networks without requiring high-density ground truth. However, performance bottlenecks within networks trained using such a formulation are observable as blurry artifacts and poor estimation around edges. To reduce these artifacts and improve predictions around edges without increasing computational complexity, we revisit the design space of underlying networks and propose modifications to improve feature aggregation and representation. We highlight inefficient feature utilization from encoders, sub-optimal upsampling, and lack of geometric constraints to restrict the fidelity of depth estimations. To overcome these, we propose (a) replacement of feature extractor within the encoder to better leverage features across multiple scales, (b) improving feature aggregation and representation within decoder via sub-pixel convolutions, and (c) integrating a lightweight attention mechanism to capture long-range dependencies. Furthermore, to enforce geometric constraints in the form of scene semantics, we propose integrating semantic segmentation as an auxiliary decoder branch and ensure feature sharing using bidirectional-cross-task attention mechanism (BCTAM). We empirically demonstrate the efficacy of the proposed formulation multiple datasets and report state-of-the-art (SoTA) performance vis-a-vis prior approaches.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="shyam2022msda" class="col-sm-8">
    
      <div class="title">Multi-Source Domain Alignment for Robust Segmentation in Unknown Targets</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Shyam, Pranjay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yoon, Kuk-Jin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kim, Kyung-Soo
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In arXiv</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Semantic segmentation provides scene understanding capability by performing pixel-wise classification of objects within an image. However, the sensitivity of such algorithms towards domain changes requires fine-tuning using an annotated dataset for each novel domain, which is expensive to construct and inefficient. We highlight that irrespective of the training dataset, structural properties of scenes remain the same hence domain sensitivity arises from training methodology. Thus, in this paper, we propose a domain alignment approach wherein multiple synthetic source domains are used to train an underlying segmentation network such that it performs consistently in unknown real target domains. Towards this end, we propose a pixel-wise supervised contrastive learning framework that enforces constraints in latent space resulting in features belonging to the same class being clustered closely and away from different classes. This approach allows for better capturing of global and local semantics while providing domain invariant properties. Our approach can be easily incorporated into prior semantic segmentation approaches without the significant computational overhead. We empirically demonstrate the efficacy of the proposed approach on GTAV \to Cityscapes, GTAV+Synthia \to Cityscapes, and GTAV+Synthia+Synscapes \to Cityscapes scenarios and report state-of-the-art (SoTA) performance without requiring access to images from the target domain.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CVPR</abbr>
    
  
  </div>

  <div id="shyam2022generic" class="col-sm-8">
    
      <div class="title">GIQE: Generic Image Quality Enhancement via N^th Order Iterative Degradation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Shyam, Pranjay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yoon, Kuk-Jin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kim, Kyung-Soo
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Visual degradations caused by motion blur, raindrop, rain, snow, illumination, and fog deteriorate image quality and, subsequently, the performance of perception algorithms deployed in outdoor conditions. While degradation-specific image restoration techniques have been extensively studied, such algorithms are domain sensitive and fail in real scenarios where multiple degradations exist simultaneously. This makes a case for blind image restoration and reconstruction algorithms as practically relevant. However, the absence of a dataset diverse enough to encapsulate all variations hinders development for such an algorithm. In this paper, we utilize a synthetic degradation model that recursively applies sets of random degradations to generate naturalistic degradation images of varying complexity, which are used as input. Furthermore, as the degradation intensity can vary across an image, the spatially invariant convolutional filter cannot be applied for all degradations. Hence to enable spatial variance during image restoration and reconstruction, we design a transformer-based architecture to benefit from the long-range dependencies. In addition, to reduce the computational cost of transformers, we propose a multi-branch structure coupled with modifications such as a complimentary feature selection mechanism and the replacement of a feed-forward network with lightweight multiscale convolutions. Finally, to improve restoration and reconstruction, we integrate an auxiliary decoder branch to predict the degradation mask to ensure the underlying network can localize the degradation information. From empirical analysis on 10 datasets covering rain drop removal, deraining, dehazing, image enhancement, and deblurring, we demonstrate the efficacy of the proposed approach while obtaining SoTA performance. </p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="shyam2022dgss" class="col-sm-8">
    
      <div class="title">DGSS : Domain Generalized Semantic Segmentation using Iterative Style Mining and Latent Representation Alignment</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Shyam, Pranjay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Bangunharcana, Antyanta,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yoon, Kuk-Jin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kim, Kyung-Soo
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In arXiv preprint arXiv:2202.13144</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/https://arxiv.org/abs/2202.13144" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Semantic segmentation algorithms require access to well-annotated datasets captured under diverse illumination conditions to ensure consistent performance. However, poor visibility conditions at varying illumination conditions result in laborious and error-prone labeling. Alternatively, using synthetic samples to train segmentation algorithms has gained interest with the drawback of domain gap that results in sub-optimal performance. While current state-of-the-art (SoTA) have proposed different mechanisms to bridge the domain gap, they still perform poorly in low illumination conditions with an average performance drop of - 10.7 mIOU. In this paper, we focus upon single source domain generalization to overcome the domain gap and propose a two-step framework wherein we first identify an adversarial style that maximizes the domain gap between stylized and source images. Subsequently, these stylized images are used to categorically align features such that features belonging to the same class are clustered together in latent space, irrespective of domain gap. Furthermore, to increase intra-class variance while training, we propose a style mixing mechanism wherein the same objects from different styles are mixed to construct a new training image. This framework allows us to achieve a domain generalized semantic segmentation algorithm with consistent performance without prior information of the target domain while relying on a single source. Based on extensive experiments, we match SoTA performance on SYNTHIA \to Cityscapes, GTAV \to Cityscapes while setting new SoTA on GTAV \to Dark Zurich and GTAV \to Night Driving benchmarks without retraining.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">RS</abbr>
    
  
  </div>

  <div id="hong2022robotic" class="col-sm-8">
    
      <div class="title">Robotic Mapping Approach under Illumination-Variant Environments at Planetary Construction Sites</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Hong, Sungchul,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shyam, Pranjay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Bangunharcana, Antyanta,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Shin, Hyuseoung
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Remote Sensing</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">paper</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>With the recent discovery of water-ice and lava tubes on the Moon and Mars along with the development of in-situ resource utilization (ISRU) technology, the recent planetary exploration has focused on rover (or lander)-based surface missions toward the base construction for long-term human exploration and habitation. However, a 3D terrain map, mostly based on orbiters’ terrain images, has insufficient resolutions for construction purposes. In this regard, this paper introduces the visual simultaneous localization and mapping (SLAM)-based robotic mapping method employing a stereo camera system on a rover. In the method, S-PTAM is utilized as a base framework, with which the disparity map from the self-supervised deep learning is combined to enhance the mapping capabilities under homogeneous and unstructured environments of planetary terrains. The overall performance of the proposed method was evaluated in the emulated planetary terrain and validated with potential results.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AAAI</abbr>
    
  
  </div>

  <div id="shyam2021towards" class="col-sm-8">
    
      <div class="title">Towards Domain Invariant Single Image Dehazing</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Shyam, Pranjay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yoon, Kuk-Jin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kim, Kyung-Soo
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
      
      <a href="https://www.aaai.org/AAAI21Papers/AAAI-1706.ShyamP.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Presence of haze in images obscures underlying information, which is undesirable in applications requiring accurate environment information. To recover such an image, a dehazing algorithm should localize and recover affected regions while ensuring consistency between recovered and its neighboring regions. However owing to fixed receptive field of convolutional kernels and non uniform haze distribution, assuring consistency between regions is difficult. In this paper, we utilize an encoder-decoder based network architecture to perform the task of dehazing and integrate an spatially aware channel attention mechanism to enhance features of interest beyond the receptive field of traditional conventional kernels. To ensure performance consistency across diverse range of haze densities, we utilize greedy localized data augmentation mechanism. Synthetic datasets are typically used to ensure a large amount of paired training samples, however the methodology to generate such samples introduces a gap between them and real images while accounting for only uniform haze distribution and overlooking more realistic scenario of non-uniform haze distribution resulting in inferior dehazing performance when evaluated on real datasets. Despite this, the abundance of paired samples within synthetic datasets cannot be ignored. Thus to ensure performance consistency across diverse datasets, we train the proposed network within an adversarial prior-guided framework that relies on a generated image along with its low and high frequency components to determine if properties of dehazed images matches those of ground truth. We preform extensive experiments to validate the dehazing and domain invariance performance of proposed framework across diverse domains and report state-of-the-art (SoTA) results.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">BMVC</abbr>
    
  
  </div>

  <div id="shyam2021lightweight" class="col-sm-8">
    
      <div class="title">Lightweight hdr camera isp for robust perception in dynamic illumination conditions via fourier adversarial networks</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Shyam, Pranjay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yoon, Kuk-Jin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sengar, Sandeep Singh,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kim, Kyung-Soo
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In The 32nd British Machine Vision Conference, BMVC</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
      
      <a href="https://www.bmvc2021-virtualconference.com/assets/papers/1406.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The limited dynamic range of commercial compact camera sensors results in an inac-
curate representation of scenes with varying illumination conditions, adversely affecting
image quality and subsequently limiting the performance of underlying image process-
ing algorithms. Current state-of-the-art (SoTA) convolutional neural networks (CNN) are
developed as post-processing techniques to independently recover under-/over-exposed
images. However, when applied to images containing real-world degradations such as
glare, high-beam, color bleeding with varying noise intensity, these algorithms amplify
the degradations, further degrading image quality. We propose a lightweight two-stage
image enhancement algorithm sequentially balancing illumination and noise removal us-
ing frequency priors for structural guidance to overcome these limitations. Furthermore,
to ensure realistic image quality, we leverage the relationship between frequency and
spatial domain properties of an image and propose a Fourier spectrum-based adversar-
ial framework (AFNet) for consistent image enhancement under varying illumination
conditions. While current formulations of image enhancement are envisioned as post-
processing techniques, we examine if such an algorithm could be extended to integrate
the functionality of the Image Signal Processing (ISP) pipeline within the camera sensor
benefiting from RAW sensor data and lightweight CNN architecture. Based on quanti-
tative and qualitative evaluations, we also examine the practicality and effects of image
enhancement techniques on the performance of common perception tasks such as object
detection and semantic segmentation in varying illumination conditions</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IJCAI-W</abbr>
    
  
  </div>

  <div id="shyam2021evaluating" class="col-sm-8">
    
      <div class="title">Evaluating COPY-BLEND Augmentation for Low Level Vision Tasks</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Shyam, Pranjay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sengar, Sandeep Singh,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yoon, Kuk-Jin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kim, Kyung-Soo
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In arXiv preprint arXiv:2103.05889</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">paper</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Image restoration and enhancement algorithms aim to improve image quality for ensuring consistent performance of high level vision tasks such as object detection, semantic segmentation, image classification, etc in different adverse conditions. State-of-the-art (SoTA) approaches have relied on brute force combination of large scale datasets, deep convolutional neural networks (CNNs) and data augmentation techniques to achieve a peak performance. However a critical question that remains unanswered is the amount of paired training dataset actually needed to reach peak performance, this is extremely critical as capturing a paired dataset corresponding to different degradations is time consuming. In this paper we focus on region modification based data augmentation techniques as well as discriminator augmentation to examine if these can improve performance of low level vision tasks such as image enhancement, dehazing, deraining, deblurring, super resolution and denoising. The primary objective being in ensuring that an underlying algorithm localizes, identifies quantum of recovery and subsequently recovers affected regions resulting in increased perceptual quality of a recovered image. From extensive experiments we summarize intriguing properties such as improved higher performance from lower amounts of training data without any modification to baseline algorithms</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICCV-W</abbr>
    
  
  </div>

  <div id="Shyam_2021_ICCV" class="col-sm-8">
    
      <div class="title">Weakly Supervised Approach for Joint Object and Lane Marking Detection</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Shyam, Pranjay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yoon, Kuk-Jin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kim, Kyung-Soo
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</em>
      
      
        Oct
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
      
      <a href="https://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Shyam_Weakly_Supervised_Approach_for_Joint_Object_and_Lane_Marking_Detection_ICCVW_2021_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/pranjay-dev/JOLD" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Understanding the driving scene is critical for the safe operation of autonomous vehicles with state-of-the-art (SoTA) systems relying upon a combination of different algorithms to perform tasks for mathematically representing an environment. Amongst these tasks, lane and object detection are highly popular and have been extensively researched independently. However, their joint operation is rarely studied primarily due to the lack of a dataset that captures these attributes together, resulting in increased redundant computations that can be eliminated simply by performing these tasks together. To overcome this, we propose a weakly-supervised approach wherein, given an image from the lane detection dataset, we use a pretrained network to label different objects within a scene, generating pseudo bounding boxes used to train a network that jointly detects objects and lane lines. With an emphasis on inference speed and performance, we utilize prior works to construct two architectures based on Convolutional Neural Networks (CNNs) and Transformers. The CNN-based approach uses row-based pixel classification to detect and cluster lane lines alongside a single-stage anchor free object detector while sharing the same encoder backbone. Alternatively, using dual decoders, the transformer-based approach directly estimates bounding boxes and polynomial coefficients of lane lines. Through extensive qualitative and quantities experiments, we demonstrate the efficacy of the proposed architectures on leading datasets for object and lane detections and report state-of-the-art (SoTA) performance per GFLOPs.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICRA</abbr>
    
  
  </div>

  <div id="9561632" class="col-sm-8">
    
      <div class="title">Adversarially-trained Hierarchical Feature Extractor for Vehicle Re-identification</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Shyam, Pranjay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yoon, Kuk-Jin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kim, Kyung-Soo
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 2021 IEEE International Conference on Robotics and Automation (ICRA)</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">paper</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Vehicle Re-identification (Re-ID) aims to retrieve all instances of query vehicle images present in an image pool. However viewpoint, illumination, and occlusion variations along with subtle differences between two unique images pose a significant challenge towards achieving an effective system. In this paper, we emphasize upon enhancing the performance of visual feature based ReID system by improving feature embedding quality and propose (1) an attention-guided hierarchical feature extractor (HFE) that leverages the structure of a backbone CNN to extract coarse and fine-grained features and (2) to train the proposed network within a hard negative adversarial framework that generates samples exhibiting extreme variations, encouraging the network to extract important distinguishing features across varying scales. To demonstrate the effectiveness of the proposed framework we use VERI-Wild, VRIC and Veri-776 datasets that exhibit extreme intra-class and minute inter-class differences and achieve state-of-the-art (SoTA) performance.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICCAS</abbr>
    
  
  </div>

  <div id="9268426" class="col-sm-8">
    
      <div class="title">Retaining Image Feature Matching Performance Under Low Light Conditions</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Shyam, Pranjay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  *Bangunharcana, Antyanta,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kim, Kyung-Soo
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 2020 20th International Conference on Control, Automation and Systems (ICCAS)</em>
      
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">paper</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Poor image quality in low light images may result in a reduced number of feature matching between images. In this paper, we investigate the performance of feature extraction algorithms in low light environments. To find an optimal setting to retain feature matching performance in low light images, we look into the effect of changing feature acceptance threshold for feature detector and adding pre-processing in the form of Low Light Image Enhancement (LLIE) prior to feature detection. We observe that even in low light images, feature matching using traditional hand-crafted feature detectors still performs reasonably well by lowering the threshold parameter. We also show that applying LLIE algorithms can improve feature matching even further when paired with the right feature extraction algorithm.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICRA</abbr>
    
  
  </div>

  <div id="9197076" class="col-sm-8">
    
      <div class="title">Dynamic Anchor Selection for Improving Object Localization</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Shyam, Pranjay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yoon, Kuk-Jin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kim, Kyung-Soo
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 2020 IEEE International Conference on Robotics and Automation (ICRA)</em>
      
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">paper</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Anchor boxes act as potential object localization candidates allow single-stage detectors to achieve real-time performance, at the cost of localization accuracy when compared to state-of-the-art two-stage detectors. Therefore, correct selection of the scale and aspect ratio associated with an anchor box is crucial for detector performance. In this work, we propose a novel architecture called DANet for improving the localization performance of single-stage object detectors, while maintaining real-time inference. The proposed network achieves this by predicting (1) the combination of aspect ratio and scale per feature map based on object density and (2) localization confidence per anchor box. We evaluate the proposed network using the benchmark dataset. On the MS COCO dataset, DANet achieves 30.9% AP at 51.8 fps using ResNet-18 and 45.3% AP at 7.4 fps using ResNeXt-101.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>


</div>

  </article>

</div>
    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    © Copyright 2022 Pranjay  Shyam.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
    Last updated: April 11, 2022.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
